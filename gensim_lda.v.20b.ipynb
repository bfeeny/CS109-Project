{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Change Log\n",
      "### v.1\n",
      "- initial build\n",
      "\n",
      "### v.2 \n",
      "- only use `isalpha()` words in dictionary/model\n",
      "- remove mispelled words by building difference of sets against `nltk.corpus.words.words()`\n",
      "\n",
      "### v.3\n",
      "- lots more notes added\n",
      "- more work on investigating spell correction routines \n",
      "\n",
      "### v.4 \n",
      "- re-ordered spellling routines\n",
      "- spell correcting is now online and part of the pipeline using `enchant` / `pyenchant`\n",
      "- I am making the assumption this code is pipelining a single school at a time\n",
      "- removing stopwords now after spelling correction\n",
      "\n",
      "### v.5\n",
      "- added variables for constants `MIN_WORD_COUNT`\n",
      "- now using `itertools.chain.from_iterable()` to flatten list of lists\n",
      "- now using a `collections.defaultdict()` (hash) to do word counts \n",
      "- word counts (due to the above two optimizations) are now near instantaneous\n",
      "- using `file_root` variable to propogate changing filenames throughout notebook\n",
      "- replaced nested comprehensions with `itertools`\n",
      "- added full parameters to LDA model\n",
      "- add logging so we can see any warnings from gensim\n",
      "- added random seed for debugging (not needed unless you want reproducable result)\n",
      "- added `dictionary.filter_extremes()` (still need to find best settings)\n",
      "\n",
      "### v.6\n",
      "- removed all LSI / LSA now just doing LDA\n",
      "- tuned LDA parameters for `passes=10` and `update_every=0` (batch mode) by default\n",
      "- added standard `matplotlib` setup from our student notebooks\n",
      "- added `remove_border()` from our student notebooks\n",
      "- added histograms for word frequency counts\n",
      "- added models for LDA from tradional bow as well as tf-idf\n",
      "- added comments at the top of most cells to give some indication of what runtime takes\n",
      "- added descriptive statistics on the dictionary\n",
      "\n",
      "### v.7\n",
      "- now using mysql database routines\n",
      "- now properly closing filehandle on file routines\n",
      "- create `read_review_file` function\n",
      "- create `clean_review_file` function\n",
      "- removed \"memory friendly\" routines which work with persistant disk\n",
      "\n",
      "### v.8\n",
      "- increased stopwords\n",
      "- add `WordNet` Lemmatization\n",
      "- removing all words `len(word) < 3` in `clean_reviews`\n",
      "- added bigram support (`CELL 110` controls whether unigrams or bigrams are used)\n",
      "- `CELL 130` and `CELL 230` will not work properly with bigrams in use so leave those commented out if using bigrams\n",
      "- `CELL 200` will not produce any output if using bigrams\n",
      "- labeled cells\n",
      "\n",
      "### v.9\n",
      "- `CELL 70` grab school names\n",
      "- `CELL 80` now pass a stop list to `clean_reviews`\n",
      "- `CELL 110` now always computing bigrams\n",
      "- added some additional stopwords to `custom_stopset`\n",
      "- `CELL 170` stitches unigram and bigram texts together for processing\n",
      "- `CELL 175` we chose which text to process: unigrams, bigrams or a unigram+bigram text\n",
      "- `CELL 210` now calling `corpus corpus_bow`\n",
      "- `CELL 240` now caling `tfidf corpus_tfidf`\n",
      "- added `CELL 265` choose a `corpus`\n",
      "- removed `CELL 280` (code factored into `CELL 270`)\n",
      "- I upgraded my own workstation to premium (free) versions of anaconda/accelerate, timings now reflect that\n",
      "- only now showing times for cells that take a while\n",
      "- moved `summary_statistics` into its own `CELL 25`\n",
      "- moved getting `clean reviews` from `CELL 90` to new `CELL 94`\n",
      "- moved descriptive statistics from `CELL 90` to new `CELL 96`\n",
      "- moved saving `corpus` from `CELL 210` to `CELL 212`\n",
      "- Added nicer headings \n",
      "- added information about installing distributed computing components\n",
      "- LDA now running in distributed mode\n",
      "\n",
      "### v.10\n",
      "- added `CELL 1500` HDP model for experimentation\n",
      "- added `MAX_NUM_REVIEWS` so you can reduce amount of reviews when testing\n",
      "- added `feature` dict to control what features are performed on text\n",
      "- added tokenize feature and step in `clean_reviews`, before this was being done when removing stopwords\n",
      "- re-worked entire `clean_reviews` function around `feature` dict\n",
      "- incorporated `feature` dict to control which cells are enabled\n",
      "- added pos tagging for later analysis\n",
      "- updated `CELL 1000` for exploratory analysis\n",
      "\n",
      "### v.11\n",
      "- added `CELL 95` to look at the text\n",
      "- corrected error in `CELL 60` where not intializing spell corrector unless `analyze_spell_correct` was set\n",
      "- now using `nltk.tokenize` for tokenization of sentences and words\n",
      "\n",
      "### v.12\n",
      "- added `CELL 8`5 to instantiate parallel processing\n",
      "- added `parallel` to `feature` dict\n",
      "- added decoration to `CELL 80` for parallel processing\n",
      "- modified `CELL 80` so it used `data_to_clean` and `stopset_to_use` as a name space hack\n",
      "- modified `CELL 90, 94` to use `data_to_clean` and `stopset_to_use`\n",
      "- modified `CELL 90` to use parallel processing for `school_names` if set\n",
      "- added `CELL 91` to merge parallel data back into single result for `school_names`\n",
      "- added parallel routines to `CELL 94`\n",
      "- `CELL 96` now renumbered to `CELL 98`\n",
      "- `CELL 95` now renumbered to `CELL 97`\n",
      "- added `CELL 96` to merge data returned from parallel workers for `texts`, `texts_uncorrected`, etc\n",
      "- move `bigram_stoplist` creation from `CELL 90` to `CELL 93`\n",
      "- `stopset_bigrams` is now just a set of bigrams of school names, nothing else\n",
      "- `CELL 97` now using `features` dict to control output\n",
      "- added `tfidf` to `features` dict to control corpus creation and dictionary transform\n",
      "- added cell magic `%%time` to `CELLS 90, 94, 96, 270`\n",
      "- added \"way\", \"thing\", and \"lot\" to `stopset_unigram`\n",
      "- factored code into `get_review_data()` for `CELL 97`\n",
      "- added `CELL 95` text diagnostics\n",
      "- tracked down bug in `CELL 80` `clean_reviews`, now imputing \"the\" instead of skipping `None` reviews in order to preserve indices across texts\n",
      "- `CELL 80` moved lowercase function after spell correct, this will allow pos tagging to work more effectively\n",
      "- `CELL 80`, factored out removal of smallwords (`len < 3`) into its own `remove_smallwords` step which can be set in `feature` dict\n",
      "- features in `features` dict are now listed in the order of processing\n",
      "- starts Notes section to notebook\n",
      "- added second pass of stopword removal after lemmatization in `CELL 80`\n",
      "- HyperThreading tested, tested with 8 virtual cores\n",
      "\n",
      "### v.13\n",
      "- added feature `only_tagged,` when set, only parts of speech in tag_set are used\n",
      "- added `tag_set` which contains part of speech tags we will use when feature `only_tagged` is set\n",
      "- removed `only_nn` as depricated by `only_tagged`\n",
      "- modified `CELL 90, 94` to distribute `tag_set` to remote workers\n",
      "- modified `CELL 80` to now use `tag_set`\n",
      "- added \"everybody\", \"everyone\", \"great\", \"excellent\" and \"part\" to stoplist\n",
      "- server setup on Amazon EC2, see Notes section for more info, entire notebook up and running fine\n",
      "- added notes about installing Anaconda's premium distro as a recommendation for faster BLAS libraries\n",
      "- `CELL 80` added conversion of POS tags from treebank/penn style to morphy style for passing into Lemmatizer (not being used yet)\n",
      "- added info about starting the Pyro nameserver to Installation\n",
      "- `clean_reviews()` is now renamed to `clean_data()` which is more reflective of what the function is doing\n",
      "\n",
      "### v.14\n",
      "- moved spellcheck routine to be above POS tagger\n",
      "- `CELL 80` map penn pos tags to morphy pos tags when pos_tag enabled for lemmatization\n",
      "- re-write `CELL 80` `clean_data` to be pos_tag aware on all routines, allowing lemmatization to use pos tags\n",
      "- added to `custom_stoplist` 'hill','valley','den','alto','crest','wood','land'\n",
      "- `CELL 1000` wrote `get_topic_data()`\n",
      "- added `perplexity_search` feature parameter\n",
      "- added perplexity grid search to `CELL 268`\n",
      "- added \"love\", \"amazing\", and \"okay\" to `custom_stoplist`\n",
      "- `CELL 97` is now `CELL 98`, `CELL 98` is now `CELL 99`\n",
      "- new `CELL 97` created to save variables `reviews, school_names, texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict`\n",
      "- added `preprocess_save` to `feature` dict.  Saves pre-processed data in `CELL 97`\n",
      "- added `preprocess_load` to `feature` dict.  Loads pre-processed data in `CELL 70` instead of raw db/file data\n",
      "- skipping `CELLS 75, 80, 85, 90, 91, 94, 95 and 96` if `preprocess_load` set\n",
      "- `CELL 269` added to review results of perplexity search\n",
      "- added `preprocess_file` variable to use as filename for loading and saving preprocessed data\n",
      "\n",
      "### v.15\n",
      "- add import re to global imports and `CELL 85`\n",
      "- stripping escape characters in `CELL 70` using a `re` compile pattern if `remove_html` is set\n",
      "- added updated `mysql_ops` class in `CELL 30`\n",
      "- modified `CELL 70` to use updated `mysql_ops` syntax\n",
      "- identified bug in `gensim`, filed on [github](https://github.com/piskvorky/gensim/issues/144#issuecomment-29562894), proposed a solution, fixed\n",
      "\n",
      "\n",
      "### v.16\n",
      "- updated formatting and flow to `CELL 268`\n",
      "- removed unigram and bigram frequency count historgrams from `CELL 150, 151`\n",
      "- merged `CELL 160` into ` CELL 150 and `CELL 161` into `CELL 151`\n",
      "- multiple additional comments added to cells\n",
      "- `CELL 1500` (HDP model) removed\n",
      "- `CELL 140` removed, as its functionality is already handled by `dictionary.filter_extremes()` in `CELL 180`\n",
      "- Now saving and loading `nces_code` and `universal_id` as `reviews_indexes` in `CELL 70` and `CELL 97`\n",
      "- Added routine to extract beta, gamma and log probabilities and save them to a file in `CELL 95`\n",
      "\n",
      "### v.17\n",
      "- code from `CELL 270` moved to new `CELL 292` to build `corpus_model`\n",
      "- modified `CELL 270` to store `num_topics` as a global variable\n",
      "- modified `CELL 290` to save model using `file_root`\n",
      "- added `CELL 295` to save `corpus_model`\n",
      "- added `CELL 310` to store `gamma` and `beta` using `file_root`\n",
      "- `CELL 97` now stores files individually using `file_root` as a base\n",
      "- modify `CELL 80` to work with situation where `pos_tag` is set but `only_tagged` is not set\n",
      "- added `dict_corpus_save` feature to control saving of dictionary and corpus in `CELL 182` and `CELL 212`\n",
      "- added `dict_corpus_load` feature to control loading of dictionary and corpus in `CELL 205`\n",
      "- added `model_save` feature to control saving of model in `CELL 290`\n",
      "- added `model_load` feature to control loading of model in `CELL 289`\n",
      "- added global variable `num_topics` in `CELL 0` which is used for creation of filenames to be loaded/saved and LDA model\n",
      "- added feature `beta_gamma_save` to save `beta` and `gamma` of model in `CELL 310`\n",
      "- added feature `model_topics_save` to save `model_topics` in `CELL 302`\n",
      "- added feature `corpus_model_save` to save `corpus_model` in `CELL 296`\n",
      "- added feature `model_topics_load` to load `model_topics` in `CELL 300`\n",
      "- added `CELL 300` to view model_topics\n",
      "- added feature `corpus_model_load` to load `corpus_model` in `CELL 293`\n",
      "- moved code from `CELL 292` to new `CELL 295` for viewing `corpus_model`\n",
      "- added feature `texts_final_load` to load `texts_final` in `CELL 160`\n",
      "- added feature `texts_final_save` to save `texts_final` in `CELL 178`\n",
      "\n",
      "### v.18\n",
      "- updated `CELL 70` to show filenames and sizes as they are loaded\n",
      "- updated `CELL 97` to show filenames and sizes as they are loaded\n",
      "\n",
      "### v.19\n",
      "- `CELL 40` which dealt with reading from a file has been removed\n",
      "- No longer using `preprocess_file` to save all texts to, instead saving each one separately\n",
      "- Removed `CELL 120` and `CELL 130` which dealt with analyzing our spell correction\n",
      "- Factored `CELL 150` and `CELL 151` into a function `frequent_tokens` in `CELL 150`\n",
      "- `CELL 151` now displays frequency counts of unigrams, `CELL 152` displays frequency counts of bigrams\n",
      "- `feature preprocess_load` and `texts_final_load` are mutually exclusive otherwise you may get errors\n",
      "- added `CELL 1100` to look at how to start getting data ready to be handed off to the database\n",
      "- numerous logic corrections in how feature's are handled and what CELLs should run\n",
      "- added `id` to `reviews_indexes` in `CELL 70`\n",
      "\n",
      "### v.20\n",
      "- added `postdate` to `reviews_indexes`\n",
      "\n",
      "\n",
      "### TODO/CONCERNS\n",
      "- factor out pickle read/write functions\n",
      "- `CELL 70` convert db functions to variables\n",
      "- Many more notes to write\n",
      "- Is the stopset ideal?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Installation\n",
      "It is recommended you install the premium version of Anaconda, Accelerate and IOPro from [Continuum](https://store.continuum.io/cshop/anaconda/).  \n",
      "\n",
      "You will see that there is an \"All Products are free for Academic Use\" link in the upper right, where you can obtain\n",
      "free licensing.  This will give you, among many other benefits, faster BLAS libraries.  You can verify your BLAS libraries here:\n",
      "\n",
      "    import numpy as np\n",
      "    np.__config__.show()\n",
      "\n",
      "You should install `gensim`\n",
      "\n",
      "    pip install gensim\n",
      "You should install `nltk`\n",
      "    \n",
      "    pip install nltk\n",
      "You will need to install specific modules from `nltk`.  They are `stopwords, punkt, wordnet, maxent_treebank_pos_tagger`. Fire up a `python` interpretor and run\n",
      "\n",
      "    import nltk\n",
      "    nltk.download()\n",
      "You should install `pymysql`\n",
      "\n",
      "    pip install pymysql\n",
      "\n",
      "If you get any errors about `error: invalid command 'egg_info'` you just need to install `setuptools`, as our original \n",
      "anaconda distribution in class used a package called `distribute` but parts of that have been factored out to `setuptools`\n",
      "\n",
      "    pip install --upgrade setuptools\n",
      "\n",
      "You should install `Enchant` for spell check capability, http://www.abisource.com/projects/enchant/ \n",
      "I used [homebrew](http://brew.sh) to install it on OSX, `brew install enchant`\n",
      "You should then install `pyenchant`\n",
      "    pip install pyenchant\n",
      "\n",
      "####iPython Parallel Processing\n",
      "iPythons parallel processing is handled through an architecture called [ipcluster](http://ipython.org/ipython-doc/dev/parallel/parallel_process.html).  We use this in our notebook to parallelize the text pre-processing pipeline.\n",
      "\n",
      "first start an ipython `ipcluster` on your machine.  For example, for 4 cores:\n",
      "\n",
      "    ipcluster start -n 4 \n",
      "\n",
      "- most chips, such as Intel, support HyperThreading, so you can usually run say 8 virtual cores on a 4 physical core chip\n",
      "- make sure `parallel` feature is set in `feature` dict\n",
      "- make sure the decoration in `CELL 80` is uncommented `@dv.remote(block=True)`\n",
      " \n",
      "you could also start the `ipcluster` right from within the iPython notebook using the \"Cluster\" tab\n",
      " \n",
      "#### Gensim Distributed Computing for LDA\n",
      "[Gensim](http://radimrehurek.com/gensim/) leverages a distributed RMI type architecture called [Pyro4](http://pythonhosted.org/Pyro4/)\n",
      "To distribute the workload to other cores or computers on the same broadcast domain\n",
      "install `gensim[distributed]`\n",
      "    \n",
      "    pip install gensim[distributed]  (or \"pip install --update gensim[distributed]\" if you already installed previously)\n",
      "this installs Pyro4 as well.  Pyro4 is the distributed framework that `gensim` utilizes.\n",
      "\n",
      "set environment variables (these must be set in the session you use to start the notebook from)\n",
      "    \n",
      "    export PYRO_SERIALIZERS_ACCEPTED=pickle\n",
      "    export PYRO_SERIALIZER=pickle\n",
      "\n",
      "start the `Pyro4` nameserver\n",
      "\n",
      "    python -m Pyro4.naming -n 0.0.0.0 &\n",
      "start workers (ex: on a four core system to start four workers)\n",
      "    \n",
      "    python -m gensim.models.lda_worker &\n",
      "    python -m gensim.models.lda_worker &\n",
      "    python -m gensim.models.lda_worker &\n",
      "    python -m gensim.models.lda_worker &\n",
      "start dispatcher (just runs on one node, can also be a worker node)\n",
      "\n",
      "    python -m gensim.models.lda_dispatcher &\n",
      "make sure in the model, you have `distributed=True` option set\n",
      "that's it, you should see info about the distributed computing in the logs as you run the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Configuration\n",
      "\n",
      "#### CELL 0\n",
      "`CELL 0` contains a number of options which control the operation of the notebook.\n",
      "\n",
      "The notebook can be navigated in an ad-hoc way, skipping cells you do not wish to use.  To make various routine tasks easier, as well as provide control over things like the text pipeline, a dictionary called `feature` is used.  This is self-documented in `CELL 0`.  An important thing to note is that many of the features could be enabled simultaneously but may not make sense or may give undesirable output.  There is little to no dependency checking. \n",
      "\n",
      "`MAX_NUM_REVIEWS` can be set.  This will control how many reviews are ingested if using the database.  If set to `0` then all reviews returned by the function being used will be included.\n",
      "\n",
      "`tag_set` can be set to control which parts of speech are included when `only_tagged` is used in conjunction with feature `pos_tag`.  The various parts of speech that can be set here are explained at [Penn Treebank Tags](http://bulba.sdsu.edu/jeanette/thesis/PennTags.html).\n",
      "\n",
      "`data_dir` can be set as the location to use when loading/saving texts, corpus, dictionaries, models, etc.\n",
      "\n",
      "`file_root` can be set, this controls the naming of all the many files that are loaded and saved.  You need to be careful what this is set to if any of the \"save\" bits are set in the `feature` dict, as you could overwrite previously saved data if not careful.\n",
      "\n",
      "`custom_stopset` is a list of words which is removed from consideration in any model processing.  This is unioned with the common set of english stopwords found in NLTK's `nltk.corpus.stopwords.words('english')` to produce `stopset_unigram`.  This is the stoplist we use to remove words for any consideration in unigram processing.\n",
      "\n",
      "`stopset_unigram` is a unioned set of the above mentioned `custom_stopset` and NTLK's `nltk.corpus.stopwords.words('english')`.\n",
      "\n",
      "#### CELL 175\n",
      "\n",
      "In `CELL 175` you choose which text will go on to be considered in LDA.  Normally this is `text_combined` which is a combination of `texts` and `texts_bigrams` produced in `CELL 170`\n",
      "\n",
      "#### CELL 180\n",
      "\n",
      "In `CELL 180` you choose what tokens make it into the dictionary. By default we have this set to drop any tokens that appear in less than 5 documents.  We also drop any token that appears in more than 50% of all documents.  We also keep only the top 100000 most frequent tokens.  These can be set as desired.\n",
      "\n",
      "#### CELL 240\n",
      "\n",
      "`CELL 240` will transform the corpus via `tfidf` if set in `feature` dict.  LDA requires that actual values be used in the bag of words.  Converting to real numbers, as with `tfidf` may be beneficial, but it is not a valid transormation for traditional LDA as published.  So this feature has been left in place since earlier experiments in this project began, but all of our work for the most part has been exclusively using Bag of Words, without any transformation, including TF-IDF.\n",
      "\n",
      "#### CELL 268\n",
      "\n",
      "`CELL 268` can be used to do a grid search over any variable you wish.  The current stable branch of `gensim` however has a bug, which has been fixed in the [pyro_threads](https://github.com/piskvorky/gensim/tree/pyro_threads) branch.  So it is recommended that you install this branch if you wish to use this feature.  Otherwise, the Pyro4 underlying framework will run out of threads.  If not doing a grid search then the current release version of gensim[distributed] will work fine.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Notes\n",
      "`CELL` numbers are used at the top of each cell just as a label so they can be tracked in changes as they are happening and discussed.\n",
      "\n",
      "#### Texts\n",
      "There are currently the following key texts created in the pipeline depending on what options are enabled.  These texts are all for diagnostic purposes, the only one actually being used by LDA is the final output `texts`.  The other texts are being created because of their place in the pipeline, allow us to see how certain transformations are performing, such as analyzing spell correction.  The order of creation is below:\n",
      "\n",
      "1. `reviews_indexes`\n",
      "2. `reviews`\n",
      "3. `school_names`\n",
      "4. `texts_uncorrected` \n",
      "5. `texts_pos`\n",
      "6. `texts_unlemmatized`\n",
      "7. `lemma_dict`\n",
      "8. `texts`\n",
      "\n",
      "A summary of each text:\n",
      "\n",
      "- `reviews_indexes` - this is a copy of the original gsid, nces_code, universal_id and postdate in a dataframe\n",
      "- `reviews` - this is the data as ingested from the file or the database\n",
      "- `school_names` - this is the school names pulled from the db AFTER they have been through pre-processing.  Since most words in school names are not in our tag set (NN or NNS for example), and many words in school names are stopped, this list is quite sparse.  It is used only for the  creation of a bigram_stopset, so that any bigrams created involving school names are not used.\n",
      "- `texts_uncorrected` - this is the text before spell correction\n",
      "- `texts_pos` - this is the text after it has been tagged with parts of speech\n",
      "- `texts_unlemmatized` - this is the text before lemmatization\n",
      "- `lemma_dict` - This is a dictionary of a set of words.  It is used just as an analysis to see what words were converted to what lemmas.  \n",
      "- `texts` - this is the final text which is used further on in the notebook and is the comprises the bulk of what is inputed into LDA\n",
      "\n",
      "There are various filters and transformations being done on the text.  Here is a workflow of where the transforms and filters are done, in relation to where the texts are being created.  All texts are created in `CELL 80 clean_data()` function.\n",
      "\n",
      "#### General Data Cleaning Pipeline:\n",
      "\n",
      "    reviews and school_names created in CELL 70\n",
      "        clean_data() called in CELL 80\n",
      "            impute \"the\" if review == None\n",
      "            remove HTML encodings and escape characters\n",
      "            tokenize documents into sentences\n",
      "            tokenize sentences into words                      * texts_uncorrected created\n",
      "            spell correct words\n",
      "            POS tag words                                      * texts_pos created\n",
      "            remove punctuation\n",
      "            remove small words (len < 3)\n",
      "            remove non-alpha words\n",
      "            lowercase all words\n",
      "            remove stopwords                                   * texts_unlemmatized created\n",
      "            lemmatize all words\n",
      "            lemma_dict created\n",
      "            remove small words (len < 3) (again)\n",
      "            remove stopwords (again)                           * texts created\n",
      "            \n",
      "\n",
      "A server has been configured on [Amazon EC2](http://aws.amazon.com/ec2/instance-types/) for our final analysis.\n",
      "\n",
      "This server is a Compute Optimized `c3.8xlarge` instance.  It has the following specifications:\n",
      "CPU: 64-bit vCPU: 32 eCPU: 108 Memory: 60 GB  Processor: Intel Xeon E5-2680"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 0\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import string\n",
      "import nltk\n",
      "import itertools\n",
      "from gensim import corpora, models, similarities\n",
      "from collections import defaultdict\n",
      "import sys\n",
      "import logging\n",
      "import random\n",
      "import operator\n",
      "import pymysql\n",
      "import time\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# below for saving/loading files\n",
      "import pickle\n",
      "# below for search/replace of escape characters\n",
      "import re\n",
      "# below two imports needed for spell correction\n",
      "import enchant\n",
      "from nltk.metrics import edit_distance\n",
      "# below for bigrams\n",
      "from nltk.collocations import BigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk import bigrams\n",
      "# below for part of speech tagging\n",
      "# uses 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
      "from nltk.corpus import wordnet\n",
      "# below for tokenizing\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "# below for lemmatizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "# set seed (only for debugging, no reason to do this unless you want repeatable result)\n",
      "# One area I did find it useful, was in experimenting with grid searches\n",
      "# np.random.seed(42)\n",
      "# random.seed(42)\n",
      "\n",
      "# setup logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Maximum number of reviews to use if loading from the database, set to 0 to use all review data requested\n",
      "# If you are just trying out this notebook, I recommend setting this to a low number like 500, 1000, 3000, etc.\n",
      "# As if the query set in CELL 70 is \"all reviews\" or something like that, it could take many hours to complete\n",
      "# text processing and model building depending on the platform it is running on.\n",
      "MAX_NUM_REVIEWS = 1000\n",
      "\n",
      "# features, listed in the order of processing\n",
      "feature = {\n",
      "\n",
      "           # Text Pre-Processing\n",
      "           # The features below concern themselves with the text pre-processing pipeline.  For example the only time we use ipythons \n",
      "           # ipcluster is during text pre-processing.  Be very careful if you have any \"save\" bits, set.  You will want to make sure \n",
      "           # that your file_root (defined below the feature dict) is set to a correct value, so that you do not overwrite data you\n",
      "           # do not wish to.\n",
      "           \n",
      "           # Note: preprocess_load and texts_final_load are mutually exclusive.  It would not make sense to set them both, there\n",
      "           # are other combinations that don't make sense that could lead to errors as well.\n",
      "           \n",
      "           'parallel'              : 1, # enables ipython parallelization using multiple cores\n",
      "           'preprocess_save'       : 0, # write preprocessed data to file for later import CELL 97\n",
      "           'preprocess_load'       : 0, # load previously saved preprocessed data from file instead of db (CELL 70)\n",
      "           'remove_html'           : 1, # remove some html encodings, not needed if using data in database\n",
      "           'tokenize'              : 1, # tokenize using NLTK's sentence and word tokenizers\n",
      "           'spell_correct'         : 1, # spell correct\n",
      "           'pos_tag'               : 1, # part of speech tag\n",
      "           'only_tagged'           : 1, # only use parts of speech types listed in tag_set\n",
      "           'remove_punctuation'    : 1, # remove punctuation\n",
      "           'remove_smallwords'     : 1, # removes words with len(word) < 3\n",
      "           'alpha_only'            : 0, # use alphabetic words only (probably not necessary)\n",
      "           'analyze_spell_correct' : 1, # save text before spell correction for later analysis in CELL 98, 120, 130\n",
      "           'lowercase'             : 1, # lowercase text\n",
      "           'remove_stopwords'      : 1, # remove stopwords \n",
      "           'lemmatize'             : 1, # lemmatize text\n",
      "           'analyze_lemmatize'     : 1, # save text before lemmatization for later analysis in CELL 98, 120 \n",
      "           'lemma_dict'            : 1, # produce a dict of lemmas with words mapped to them in CELL 100 (time consuming)\n",
      "           'texts_final_load'      : 0, # load the texts_final\n",
      "           'texts_final_save'      : 0, # save the texts_final\n",
      "           \n",
      "           # Dictionary / Corpus\n",
      "           # The features below concern themselves with the creation of a dictionary and corpus.\n",
      "           \n",
      "           'dict_corpus_load'      : 0, # load the dictionary and corpus in CELL 205\n",
      "           'dict_corpus_save'      : 0, # save the dictionary created in CELL 182 and the corpus created in CELL 212\n",
      "           'tfidf'                 : 0, # TF-IDF transform corpus, otherwise leave as bag of words\n",
      "           \n",
      "           # LDA\n",
      "           # The features below concer themselves with the actual creation of the LDA model.  This is where the Pyro4\n",
      "           # framework would be used if available.\n",
      "           \n",
      "           'perplexity_search'     : 0, # enables perplexity grid search in CELL 268 rather than regular model in 270\n",
      "           'model_load'            : 0, # load the model in CELL 289\n",
      "           'model_save'            : 0, # save the model in CELL 290\n",
      "           'corpus_model_load'     : 0, # load the corpus model in CELL 293\n",
      "           'corpus_model_save'     : 0, # save the corpus model in CELL 295\n",
      "           'beta_gamma_save'       : 0, # save the LDA models beta and gamma in CELL 310\n",
      "           'model_topics_load'     : 0, # load model_topics in CELL 301\n",
      "           'model_topics_save'     : 0  # save model_topics in CELL 302\n",
      "           }\n",
      "\n",
      "# which parts of speech tags to use when only_tagged feature enabled\n",
      "# for explaination of tags http://bulba.sdsu.edu/jeanette/thesis/PennTags.html\n",
      "tag_set = ['NN','NNS']\n",
      "\n",
      "# directory to look for data files\n",
      "data_dir = \"data\"\n",
      "\n",
      "# file root name (do not add file extension).  This is used to build almost every filename used in the notebook, whether you are\n",
      "# loading or saving files.  Its important to set it correctly if you have any of the \"save\" bits set in the feature matrix, \n",
      "# otherwise you could overwrite previously saved data.\n",
      "file_root = \"CA_freq\"\n",
      "\n",
      "# number of topics to use in LDA model.  This is set hear as opposed to just in CELL 270 where the model runs, because it is used\n",
      "# in the creation of many filenames which can be saved/loaded (set in feature dict) that are specific to the number of \n",
      "# topics of the model.\n",
      "num_topics = 75\n",
      "\n",
      "# our stopsets\n",
      "custom_stopset = ['school','teachers','students','children','parents','kids','child','parent','son','sons','student','schools',\n",
      "                  'daughter','daughters',\"I'm\",\"i'm\",\"I've\",\"i've\",'teacher','principal','principle','mr','Mr','ms','Mrs',\n",
      "                  'dint','kindergarten','would','went','get','one','ones','st','make','year','years','aw','grandson','ma',\n",
      "                  \"La's\",'la','way','thing','lot','everybody','everyone','great','excellent','part','hill','valley','den',\n",
      "                  'alto','crest','wood','land','love','amazing','okay']\n",
      "\n",
      "stopset_unigram = set(nltk.corpus.stopwords.words('english'))\n",
      "stopset_unigram = stopset_unigram.union(set(custom_stopset))\n",
      "stopset_bigrams = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 10\n",
      "# Standard setup from our previous student notebooks\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 20\n",
      "# from our previous student notebooks\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 25\n",
      "# summary statistics\n",
      "def summary_stats(texts):\n",
      "    \"\"\"\n",
      "    Produces Summary Statistics on the texts provided\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    texts : A tokenized set of documents (list of lists)\n",
      "    \"\"\"\n",
      "    token_count = defaultdict(int)\n",
      "    for word in itertools.chain.from_iterable(texts):\n",
      "        token_count[word] += 1\n",
      "    print \"%d words appear in the text just 1 time\" % sum([1 for word in token_count if token_count[word] == 1])\n",
      "    print \"%d words appear in the text just 2 times\" % sum([1 for word in token_count if token_count[word] == 2])\n",
      "    n, min_max, mean, var, skew, kurt = sp.stats.describe(token_count.values())\n",
      "    print(\"Number of unique words: {0:d}\".format(n))\n",
      "    print(\"Minimum freq: {0:8d} Maximum freq: {1:8d}\".format(min_max[0], min_max[1]))\n",
      "    print(\"Mean: {0:8.2f}\".format(mean))\n",
      "    print(\"Std. deviation : {0:8.6f}\".format(sp.std(token_count.values())))\n",
      "    print(\"Variance: {0:8.2f}\".format(var))\n",
      "    print(\"Skew : {0:8.2f}\".format(skew))\n",
      "    print(\"Kurtosis: {0:8.2f}\".format(kurt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 30\n",
      "# MySQL functions\n",
      "\n",
      "\"\"\"\n",
      "class mysql_ops\n",
      "\n",
      "functions:\n",
      "\n",
      "    mysql_connect - connects to our MySQL db\n",
      "    mysql_disconnect - disconnects\n",
      "    get_mysql_data - generic retrieval function that invokes a stored procedure returning a denormalized df combining school and review data\n",
      "    \n",
      "    retrieval methods (commented inline)\n",
      "    \n",
      "\"\"\"\n",
      "class mysql_ops(object):\n",
      "    \n",
      "    # returns a connection to our MySQL db\n",
      "    @classmethod\n",
      "    def mysql_connect(self):\n",
      "        \n",
      "        cnx = pymysql.connect(host='cs109instance.ccikshmkulj7.us-east-1.rds.amazonaws.com', \n",
      "                               port=3306, \n",
      "                               user='michael', \n",
      "                               passwd='michael', \n",
      "                               db='cs109gs')\n",
      "        this_cursor = cnx.cursor(pymysql.cursors.DictCursor)\n",
      "        return cnx, this_cursor\n",
      "    \n",
      "    # close cursor and MySQL connection\n",
      "    @classmethod\n",
      "    def mysql_disconnect(self, cnx, cursor):\n",
      "        cursor.close()\n",
      "        cnx.close() # close db connection\n",
      "        \n",
      "    # get data from mysql db\n",
      "    def get_mysql_data(self, sql, data_id1=None, data_id2=None):\n",
      "        \n",
      "        cnx, cursor = self.mysql_connect()\n",
      "        \n",
      "        if data_id2:\n",
      "            cursor.callproc(sql, (data_id1, data_id2))\n",
      "        elif data_id1:\n",
      "            cursor.callproc(sql, (data_id1,))\n",
      "        else:\n",
      "            cursor.callproc(sql)\n",
      "        \n",
      "        if (cursor.rowcount > 0):\n",
      "            df = pd.DataFrame(cursor.fetchall())\n",
      "            self.mysql_disconnect(cnx, cursor)# close db connection\n",
      "            return df\n",
      "        else:\n",
      "            self.mysql_disconnect(cnx, cursor)# close db connection\n",
      "            print \"no rows returned\"\n",
      "        \n",
      "        return None\n",
      "    \n",
      "    # return all reviews for one school gsid (denormalized, i.e. with the associated school data repeated per review)\n",
      "    def get_reviews_gsid(self, gsid):\n",
      "        sql = 'school_reviews_by_gsid'\n",
      "        return self.get_mysql_data(sql, gsid)\n",
      "     \n",
      "    # return ethnic composition data for one school gsid \n",
      "    def get_race_gsid(self, gsid):\n",
      "        sql = 'school_race_by_gsid'\n",
      "        return self.get_mysql_data(sql, gsid)\n",
      "    \n",
      "    # return GS census data for one school gsid \n",
      "    def get_census_gsid(self, gsid):\n",
      "        sql = 'school_census_by_gsid'\n",
      "        return self.get_mysql_data(sql, gsid)\n",
      "    \n",
      "    # return GS test results data for one school gsid \n",
      "    def get_results_gsid(self, gsid):\n",
      "        sql = 'school_results_by_gsid'\n",
      "        return self.get_mysql_data(sql, gsid)\n",
      " \n",
      "    # return all reviews for one school ncesid (denormalized)\n",
      "    def get_reviews_ncesid(self, ncesid):\n",
      "        sql = 'school_reviews_by_ncesid'\n",
      "        return self.get_mysql_data(sql, ncesid)\n",
      "    \n",
      "    # return all reviews for one school ncesid \n",
      "    def get_reviews_districtncesid(self, districtncesid):\n",
      "        sql = 'school_reviews_by_districtncesid'\n",
      "        return self.get_mysql_data(sql, districtncesid)\n",
      "    \n",
      "    # return all reviews for one state \n",
      "    def get_reviews_state(self, state):\n",
      "        sql = 'school_reviews_by_state'\n",
      "        return self.get_mysql_data(sql, state)\n",
      "    \n",
      "     # return all census data for one state \n",
      "    def get_census_state(self, state):\n",
      "        sql = 'school_census_by_state'\n",
      "        return self.get_mysql_data(sql, state)\n",
      "    \n",
      "     # return all test results for one state \n",
      "    def get_results_state(self, state):\n",
      "        sql = 'school_results_by_state'\n",
      "        return self.get_mysql_data(sql, state)\n",
      "    \n",
      "     # return all test results for one state and year\n",
      "    def get_results_state_year(self, state, year):\n",
      "        sql = 'school_results_by_state_year'\n",
      "        return self.get_mysql_data(sql, state, year)\n",
      "    \n",
      "    # return all reviews for one county\n",
      "    def get_reviews_county(self, state, county):\n",
      "        sql = 'school_reviews_by_county'\n",
      "        return self.get_mysql_data(sql, state, county)\n",
      "    \n",
      "    # return all reviews containing text string\n",
      "    def get_reviews_string(self, text_string):\n",
      "        sql = 'school_reviews_by_string'\n",
      "        return self.get_mysql_data(sql, text_string)\n",
      "    \n",
      "    # note the psql syntax: returns all schools / reviews in db \n",
      "    # CAREFUL WITH THIS ONE:  IT WILL TAKE A LONG TIME AND RETURN EVERYTHING!\n",
      "    def get_all_reviews(self):\n",
      "        sql = 'school_reviews' \n",
      "        return self.get_mysql_data(sql)\n",
      "        #df = psql.frame_query(sql, cn)\n",
      "        #cn = cnx.mysql_disconnect()\n",
      "        #return df\n",
      "    \n",
      "    #return all cities and towns for a given state\n",
      "    def get_cities_towns_by_state(self, state):\n",
      "        sql = 'get_cities_towns_by_state' \n",
      "        return self.get_mysql_data(sql, state)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 50\n",
      "# Spell Checker Class\n",
      "\n",
      "# SpellingReplacer class taken from Python Text Processing with NLTK 2.0 Cookbook\n",
      "# It will return the word if its in the dictionary, otherwise return the closest word it can match if <= self.max_dist\n",
      "# Otherwise, it will return the word\n",
      "class SpellingReplacer(object):\n",
      "    \n",
      "    def __init__(self, dict_name='en_US', max_dist=2):\n",
      "        self.spell_dict = enchant.Dict(dict_name)\n",
      "        self.max_dist = 2\n",
      "    def replace(self, word):\n",
      "        if self.spell_dict.check(word):\n",
      "            return word\n",
      "        suggestions = self.spell_dict.suggest(word)\n",
      "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
      "            return suggestions[0]\n",
      "        else:\n",
      "            return word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 60\n",
      "# We instantiate a SpellingReplacer, and do a quick test\n",
      "if(feature['spell_correct']):\n",
      "    replacer = SpellingReplacer()\n",
      "    replacer.replace('cookbok')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 70\n",
      "# if preprocess_load is set get reviews as well as other data from pre-saved files\n",
      "# otherwise get reviews data either by db\n",
      "\n",
      "if(feature['preprocess_load']):\n",
      "    filename = data_dir + '/' + file_root + '-reviews_indexes.pickle'\n",
      "    with open(filename) as f:\n",
      "        reviews_indexes = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(reviews_indexes))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-reviews.pickle'\n",
      "    with open(filename) as f:\n",
      "        reviews = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(reviews))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-school_names.pickle'\n",
      "    with open(filename) as f:\n",
      "        school_names = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(school_names))\n",
      "    \n",
      "    filename = data_dir + '/' + file_root + '-texts.pickle'\n",
      "    with open(filename) as f:\n",
      "        texts = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(texts))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-texts_uncorrected.pickle'\n",
      "    with open(filename) as f:\n",
      "        texts_uncorrected = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(texts_uncorrected))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-texts_unlemmatized.pickle'\n",
      "    with open(filename) as f:\n",
      "        texts_unlemmatized = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(texts_unlemmatized))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-texts_pos.pickle'\n",
      "    with open(filename) as f:\n",
      "        texts_pos = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(texts_pos))\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-lemma_dict.pickle'\n",
      "    with open(filename) as f:\n",
      "        lemma_dict = pickle.load(f)\n",
      "    print \"Loaded file %s of size %d\" % (filename,len(lemma_dict))\n",
      "\n",
      "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    # instance mysql_ops\n",
      "    cnx = mysql_ops()\n",
      "    \n",
      "    # Here is where we set what query we wish to use.  It can be one that involves a constrain using MAX_NUM_REVIEWS or not.\n",
      "    # You can add whatever query you like, a full list are available in CELL 30.  Below just shows \"get_all_reviews()\" and\n",
      "    # \"get_all_reviews_state\" as those would likely be the most commonly used with this notebook.\n",
      "    if(MAX_NUM_REVIEWS == 0):\n",
      "        # reviews = cnx.get_all_reviews()[['id','nces_code','universal_id','postdate','name','reviews']]\n",
      "        reviews = cnx.get_reviews_state('CA')[['id','nces_code','universal_id','postdate','name','reviews']]\n",
      "    else:\n",
      "        # reviews = cnx.get_all_reviews()[['id','nces_code','universal_id','postdate','name','reviews']][:MAX_NUM_REVIEWS]\n",
      "        reviews = cnx.get_reviews_state('CA')[['id','nces_code','universal_id','postdate','name','reviews']][:MAX_NUM_REVIEWS]\n",
      "        # reviews = cnx.get_reviews_state('CA')[:MAX_NUM_REVIEWS]\n",
      "    # We save off the indexes of each review so we can correlate everything later if needed\n",
      "    reviews_indexes = reviews[['id','nces_code','universal_id','postdate']]\n",
      "    \n",
      "    # This is a list of school names, which we immediately reduce to a set().  Its only being used to create bigrams from\n",
      "    # which will them be added to a special stopset for bigrams.  Here they are unprocessed by after CELL 90 they will be \n",
      "    # very different looking, and very sparse, as most will have been removed during pre-processing (most parts of school names\n",
      "    # are either stopped in our pre-processing or are not parts of speech that we decide to keep, nouns for example)\n",
      "    school_names = list(set(reviews['name']))\n",
      "    \n",
      "    # This is the core of what we are processing and building our model on.  The unprocessed data from the database.\n",
      "    reviews = reviews['reviews']\n",
      "    print \"Ingested %d documents from database\" % len(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ingested 1000 documents from database\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reviews.irow(1)\n",
      "# reviews[reviews['postdate'] == '2004-07-29T00:00:00.000-07:00']\n",
      "# reviews['postdate'].str.startswith(\"2010-\")\n",
      "# reviews['postdate'].str.slice(0,4)\n",
      "# indexes = reviews[reviews['postdate'].str.slice(0,4) < '2010'].index\n",
      "# reviews[indexes.values]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = data_dir + '/' + file_root + '-reviews_indexes.pickle'\n",
      "print \"Writing file %s of size %d\" % (filename,len(reviews_indexes))\n",
      "with open(filename, 'w') as f:\n",
      "    pickle.dump(reviews_indexes, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 75\n",
      "# setup ipython for parallel processing\n",
      "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    from IPython.parallel import Client\n",
      "\n",
      "    # first start an ipython cluster on your machine.  For example, for 4 cores:\n",
      "    #    ipcluster start -n 4 \n",
      "\n",
      "    # Setup client instance\n",
      "    rc = Client()\n",
      "    print \"Discovered %d cores\" % len(rc)\n",
      "    rc.ids\n",
      "\n",
      "    # we will use a DirectView object for direct execution across all cores\n",
      "    # all your cores are belong to us\n",
      "    dv = rc[:]\n",
      "\n",
      "    # We will block on all executions\n",
      "    dv.block=True\n",
      "\n",
      "    dv.scatter('partition_ids', range(len(rc)))\n",
      "    %px print(partition_ids)\n",
      "    %px partition_id = partition_ids[0]\n",
      "    %px print(partition_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Discovered 32 cores\n",
        "[stdout:0] [0]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:1] [1]\n",
        "[stdout:2] [2]\n",
        "[stdout:3] [3]\n",
        "[stdout:4] [4]\n",
        "[stdout:5] [5]\n",
        "[stdout:6] [6]\n",
        "[stdout:7] [7]\n",
        "[stdout:8] [8]\n",
        "[stdout:9] [9]\n",
        "[stdout:10] [10]\n",
        "[stdout:11] [11]\n",
        "[stdout:12] [12]\n",
        "[stdout:13] [13]\n",
        "[stdout:14] [14]\n",
        "[stdout:15] [15]\n",
        "[stdout:16] [16]\n",
        "[stdout:17] [17]\n",
        "[stdout:18] [18]\n",
        "[stdout:19] [19]\n",
        "[stdout:20] [20]\n",
        "[stdout:21] [21]\n",
        "[stdout:22] [22]\n",
        "[stdout:23] [23]\n",
        "[stdout:24] [24]\n",
        "[stdout:25] [25]\n",
        "[stdout:26] [26]\n",
        "[stdout:27] [27]\n",
        "[stdout:28] [28]\n",
        "[stdout:29] [29]\n",
        "[stdout:30] [30]\n",
        "[stdout:31] [31]\n",
        "[stdout:0] 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:1] 1\n",
        "[stdout:2] 2\n",
        "[stdout:3] 3\n",
        "[stdout:4] 4\n",
        "[stdout:5] 5\n",
        "[stdout:6] 6\n",
        "[stdout:7] 7\n",
        "[stdout:8] 8\n",
        "[stdout:9] 9\n",
        "[stdout:10] 10\n",
        "[stdout:11] 11\n",
        "[stdout:12] 12\n",
        "[stdout:13] 13\n",
        "[stdout:14] 14\n",
        "[stdout:15] 15\n",
        "[stdout:16] 16\n",
        "[stdout:17] 17\n",
        "[stdout:18] 18\n",
        "[stdout:19] 19\n",
        "[stdout:20] 20\n",
        "[stdout:21] 21\n",
        "[stdout:22] 22\n",
        "[stdout:23] 23\n",
        "[stdout:24] 24\n",
        "[stdout:25] 25\n",
        "[stdout:26] 26\n",
        "[stdout:27] 27\n",
        "[stdout:28] 28\n",
        "[stdout:29] 29\n",
        "[stdout:30] 30\n",
        "[stdout:31] 31\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 80\n",
      "# remove punctuation, remove stopwords, remove words < 3, remove non alpha words, spell correct, lemmatize\n",
      "# you need to comment out the below decorator if you don't want to run parallel\n",
      "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "\n",
      "    @dv.remote(block=True)\n",
      "    def clean_data():\n",
      "    \n",
      "        replacer = SpellingReplacer()\n",
      "\n",
      "        texts = []\n",
      "        texts_uncorrected = []\n",
      "        texts_unlemmatized = []\n",
      "        texts_pos = []\n",
      "        lemma_dict = defaultdict(set)\n",
      "        counter = 0\n",
      "    \n",
      "        # we need to strip escape characters so we compile a pattern\n",
      "        hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
      "        \n",
      "        # Instantiate a WordNet Lemmatizer\n",
      "        wnl = WordNetLemmatizer()\n",
      "        \n",
      "        # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
      "        # http://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk\n",
      "        morphy_tag = {'NN':wordnet.NOUN,'JJ':wordnet.ADJ,'VB':wordnet.VERB,'RB':wordnet.ADV}\n",
      "        \n",
      "        print \"Starting to process %d documents\" % len(data_to_clean)\n",
      "    \n",
      "        for review_text in data_to_clean:\n",
      "            \n",
      "            # print result to stdout, note that when using parallel stdout is not sent to the Client\n",
      "            if(not feature['parallel']):\n",
      "                counter += 1\n",
      "                if ((counter % 1000)==0):\n",
      "                    print \"%d documents processed\" % counter\n",
      "                sys.stdout.flush()\n",
      "                          \n",
      "            # If its empty, impute \"the\" which is ignored anyways.  This needs to be done vs. skipping in order\n",
      "            # to keep our indexes 1:1 across all texts for analysis\n",
      "            if review_text == None:\n",
      "                review_text = \"the\"\n",
      "        \n",
      "            # remove html encodings\n",
      "            if(feature['remove_html']):\n",
      "                review_text = review_text.replace('&amp;','').replace('&lt;','').replace('&gt;','').replace('&quot;','').replace('&#039;','').replace('&#034;','')\n",
      "                review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
      "          \n",
      "            # tokenize, you don't want to turn this off\n",
      "            if(feature['tokenize']):\n",
      "                review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
      "            else:\n",
      "                review_text = [word for word in review_text.split()]\n",
      "            \n",
      "            # Below line is just to store uncorrected text so we can check spelling routines later\n",
      "            if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
      "                texts_uncorrected.append(review_text)\n",
      "            \n",
      "            # Spell correction using the Enchant library\n",
      "            # The corrected words that are returned may be capitalized as part of the correction, or even have\n",
      "            # things like apostrophes added.  We will just leave these in place for now as this does not effect\n",
      "            # the data quality. What is important is at this point the data has been de-duplicated, corrected and\n",
      "            # is consistant.  So instead of \"california\", \"California\", \"Califonria\", it may all be just \"California\"\n",
      "            # Note that spell correction DOES remove punctuation!\n",
      "            if(feature['spell_correct']):\n",
      "                review_text =  [word for words in review_text for word in replacer.replace(words).split()]\n",
      "                   \n",
      "            # get parts of speech\n",
      "            if(feature['pos_tag']):\n",
      "                review_text_pos = [word for word in nltk.pos_tag(review_text)]\n",
      "                texts_pos.append(review_text_pos)\n",
      "            \n",
      "                # Remove anything that is not in our tag_set\n",
      "                if(feature['only_tagged']):\n",
      "                    # review_text = [word[0] for word in review_text_pos if word[1] in tag_set]\n",
      "                    review_text = [word for word in review_text_pos if word[1] in tag_set]\n",
      "                else:\n",
      "                    review_text = [word for word in review_text_pos]\n",
      "    \n",
      "            # remove punctuation\n",
      "            if(feature['remove_punctuation']):\n",
      "                if(feature['pos_tag']):\n",
      "                    review_text = [(word[0].translate(string.maketrans(\"\",\"\"), string.punctuation),word[1]) for word in review_text]\n",
      "                else:\n",
      "                    review_text = [word.translate(string.maketrans(\"\",\"\"), string.punctuation) for word in review_text]\n",
      "                        \n",
      "            # remove smallwords\n",
      "            if(feature['remove_smallwords']):\n",
      "                if(feature['pos_tag']):\n",
      "                    review_text = [(word[0],word[1]) for word in review_text if not len(word[0]) < 3]\n",
      "                else:\n",
      "                    review_text = [word for word in review_text if not len(word) < 3]\n",
      "                \n",
      "            # Only consider alpha words (probably uncessary)\n",
      "            if(feature['alpha_only']):\n",
      "                if(feature['pos_tag']):\n",
      "                    review_text = [(word[0],word[1]) for word in review_text if word[0].isalpha()]\n",
      "                else:\n",
      "                    review_text = [word for word in review_text if word.isalpha()]\n",
      "            \n",
      "            # lowercase all text\n",
      "            if(feature['lowercase']):\n",
      "                if(feature['pos_tag']):\n",
      "                    review_text = [(word[0].lower(),word[1]) for word in review_text]\n",
      "                else:\n",
      "                    review_text = [word.lower() for word in review_text]\n",
      "            \n",
      "            # remove stopwords\n",
      "            if(feature['remove_stopwords']):\n",
      "                if(feature['pos_tag']):\n",
      "                    review_text = [word for word in review_text if not word[0] in stopset_to_use]\n",
      "                else:\n",
      "                    review_text = [word for word in review_text if not word in stopset_to_use]\n",
      "            \n",
      "            # Below line is just to store unlemmatized text so we can check statistics later\n",
      "            if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
      "                if(feature['pos_tag']):\n",
      "                    texts_unlemmatized.append([word[0] for word in review_text])\n",
      "                else:\n",
      "                    texts_unlemmatized.append(review_text)\n",
      "                    \n",
      "            # Lemmatize using the Wordnet Lemmatizer\n",
      "            # Use nltk's lemmatizer to create word stems\n",
      "            if(feature['lemmatize'] and not feature['lemma_dict']):\n",
      "                if(feature['pos_tag']):\n",
      "                    # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
      "                    review_text = [wnl.lemmatize(word[0],morphy_tag[word[1][:2]]) for word in review_text]\n",
      "                else:\n",
      "                    review_text = [wnl.lemmatize(word) for word in review_text]\n",
      "                \n",
      "            # creating a dictionary of lemmas can take a while, you may wish to first try on a small set by setting MAX_NUM_REVIEWS\n",
      "            # in CELL 0\n",
      "            if(feature['lemmatize'] and feature['lemma_dict']):\n",
      "                lemma_text = []\n",
      "                for word in review_text:\n",
      "                    if(feature['pos_tag']):\n",
      "                        orig_word = word[0]\n",
      "                        lemma = wnl.lemmatize(word[0],morphy_tag[word[1][:2]])\n",
      "                    else:\n",
      "                        orig_word = word\n",
      "                        lemma = wnl.lemmatize(word)\n",
      "                        \n",
      "                    # if word doesn't match its lemma then add this word to our lemma_dict\n",
      "                    if lemma != orig_word:\n",
      "                        lemma_dict[lemma].add(orig_word)\n",
      "                        \n",
      "                    lemma_text.append(lemma)\n",
      "                \n",
      "                review_text = lemma_text\n",
      "    \n",
      "            # remove stopwords and smallwords a second time, to make sure none were introduced from lemmatization \n",
      "            if(feature['lemmatize']):\n",
      "                if(feature['remove_smallwords']):\n",
      "                    review_text = [word for word in review_text if not len(word) < 3]\n",
      "    \n",
      "                if(feature['remove_stopwords']):\n",
      "                    review_text = [word for word in review_text if not word in stopset_to_use]\n",
      "    \n",
      "                        \n",
      "            # if pos_tag is turned on but lemma is not, then our text still has tags and we need to remove them\n",
      "            if(feature['pos_tag'] and not feature['lemmatize']):\n",
      "                review_text = [word[0] for word in review_text]\n",
      "            \n",
      "            # add to our list\n",
      "            texts.append(review_text)\n",
      "        return texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 85\n",
      "# setup clean_data for parallel processing\n",
      "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "\n",
      "    # We sychronize our imports to our remote workers, as they have seperate environments than ours\n",
      "    # these are just the imports that are needed for clean_data()\n",
      "    with dv.sync_imports():\n",
      "        import nltk\n",
      "        import sys\n",
      "        import string\n",
      "        import enchant\n",
      "        import re\n",
      "        from collections import defaultdict\n",
      "        from nltk.stem import WordNetLemmatizer\n",
      "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "        from nltk.metrics import edit_distance\n",
      "        from nltk.corpus import wordnet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "importing nltk on engine(s)\n",
        "importing sys on engine(s)\n",
        "importing string on engine(s)\n",
        "importing enchant on engine(s)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "importing re on engine(s)\n",
        "importing defaultdict from collections on engine(s)\n",
        "importing WordNetLemmatizer from nltk.stem on engine(s)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "importing word_tokenize,sent_tokenize from nltk.tokenize on engine(s)\n",
        "importing edit_distance from nltk.metrics on engine(s)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "importing wordnet from nltk.corpus on engine(s)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# CELL 90\n",
      "\n",
      "# %%time reports Wall time: 4.19 s for my Quad Core 2.66Ghz mac pro in parallel mode\n",
      "# %%time reports Wall time: 1min 2s for Amazon instance using ALL reviews data ~900k\n",
      "\n",
      "# get school names and create bigrams from them, then add those bigrams to our bigram stopset\n",
      "\n",
      "# We clean our school_names the exact same way we clean other data, this must be done for them be used\n",
      "# properly as stop words\n",
      "\n",
      "# You will NOT see stdout from the workers if using parallel processing\n",
      "\n",
      "# The commands below make use of the global name space as opposed to passing variables to the function. The nature of ipythons\n",
      "# paralell architecture is that it basically relies on global variables in its own environment.  We rename them here to \n",
      "# keep things organized\n",
      "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    \n",
      "    data_to_clean = school_names\n",
      "    stopset_to_use = stopset_unigram\n",
      "    \n",
      "    # copy the data_to_clean and stopset_to_use into name space of remote workers\n",
      "    if(feature['parallel']):\n",
      "        \n",
      "        # We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
      "        dv.scatter('data_to_clean',data_to_clean)\n",
      "        \n",
      "        # We copy the objects feature, stopset, tag_set as well as the Class SpellingReplacer to our remote workers\n",
      "        dv['feature'] = feature\n",
      "        dv['stopset_to_use'] = stopset_to_use\n",
      "        dv['tag_set'] = tag_set\n",
      "        dv['SpellingReplacer'] = SpellingReplacer\n",
      "        \n",
      "        # Show size of entire data being sent for processing\n",
      "        print \"Total data size: %d\" % len(data_to_clean)\n",
      "        \n",
      "        # Show the shard size of each remote worker\n",
      "        print \"Sharded data size for each worker\"\n",
      "        %px print len(data_to_clean)\n",
      "    \n",
      "        # clean the school_names\n",
      "        result = clean_data()\n",
      "    else:\n",
      "        # We are not running parallel and we only care about the first text returned from clean_data()\n",
      "        school_names = clean_data()[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 91\n",
      "# Merge Parallel Data back from remote workers back into one set for school_names\n",
      "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    school_names = []\n",
      "\n",
      "    for worker in result:\n",
      "        school_names.extend(worker[0])\n",
      "\n",
      "    print \"Total data size after merge: %d\" % len(school_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 93\n",
      "# Create stopset_bigrams from bigrams created using school names for creation of a bigram stopset\n",
      "\n",
      "# We create bigrams from the school_names, union with the original stopset to create our final bigram stopset\n",
      "\n",
      "# We only stop bigrams made from school names.  Not unigrams.  This is because there are too many words potentially used in school \n",
      "# names which we would not want to stop.  For example the word \"Art\", or \"Science\" or any number of other words.  When discussing \n",
      "# a school in a review however, reviewers repeatedly mention school names, for example if the school name were \"Glenn Brook Middle\n",
      "# School\" they may constantly mention \"Glenn Brook\".......this gets rid of it.  The reality is, when working with a tag_set of just\n",
      "# nounds (NN/NNS), most names of schools are removed from the text anyways, as they are typically Proper Nouns (NNP) or adjectives\n",
      "# and never survive further along to give us any trouble.  However having this bigram creation and stoplist is very useful if you \n",
      "# are NOT limiting to just nouns, so it give us great flexibility.\n",
      "if(not feature['texts_final_load']):\n",
      "    school_names_bigram = [[\" \".join(bigram) for bigram in bigrams(text)] for text in school_names]\n",
      "    school_names_bigram = list(itertools.chain.from_iterable(school_names_bigram))\n",
      "    stopset_bigrams = set(school_names_bigram)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# CELL 94 \n",
      "# collect optional uncorrected, unlemmatized, pos tagged and lemma dict texts for later analysis\n",
      "\n",
      "# time 1m 56s for 3000 reviews on my 2.66Ghz quad core xeon mac pro\n",
      "# time 25m for 37349 reviews on my 2.66Ghz quad core xeon mac pro\n",
      "# time 18m 18s for 37349 reviews on my 2.66Ghz quad core xeon mac pro with 8 virtual cores HyperThreading\n",
      "# time 31m 20s for 166610 CA reviews on Amazon EC2 16 hyperthreaded server\n",
      "# time 2h 15m for 890000 reviews of all schools on Amazon EC2 32 hyperthreaded server \n",
      "\n",
      "# get our cleaned reviews\n",
      "# We can do this using parallel processing or not\n",
      "# You will NOT see stdout from the workers if using parallel processing\n",
      "\n",
      "# The commands below make use of the global name space in each worker as opposed to passing variables to the function. The nature of\n",
      "# ipythons paralell architecture is that it basically utilizes separate python processes, each with their own global namespace \n",
      "if(not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    \n",
      "    data_to_clean = reviews\n",
      "    stopset_to_use = stopset_unigram\n",
      "    \n",
      "    # copy the data_to_clean and stopset_to_use into name space of remote workers\n",
      "    if(feature['parallel']):\n",
      "        \n",
      "        # We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
      "        dv.scatter('data_to_clean',data_to_clean)\n",
      "        \n",
      "        # We copy the objects feature and stopset, as well as the Class SpellingReplacer to our remote workers\n",
      "        dv['feature'] = feature\n",
      "        dv['stopset_to_use'] = stopset_to_use\n",
      "        dv['tag_set'] = tag_set\n",
      "        dv['SpellingReplacer'] = SpellingReplacer\n",
      "        \n",
      "        # Show size of entire data being sent for processing\n",
      "        print \"Total data size: %d\" % len(data_to_clean)\n",
      "        \n",
      "        # Show the shard size of each remote worker\n",
      "        print \"Sharded data size for each worker\"\n",
      "        %px print len(data_to_clean)\n",
      "    \n",
      "        # clean the reviews\n",
      "        result = clean_data()\n",
      "    else:\n",
      "        # run non-parallel\n",
      "        texts,texts_uncorrected,texts_unlemmatized,texts_pos,lemma_dict = clean_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 95\n",
      "# Diagnostics to to make sure all of our data is here.  There should be no drift in the lengths of texts.\n",
      "\n",
      "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "\n",
      "    texts_size = 0\n",
      "    texts_uncorrected_size = 0\n",
      "    texts_unlemmatized_size = 0\n",
      "    texts_pos_size = 0\n",
      "    lemma_dict_size = 0\n",
      "\n",
      "    # iterate through each worker and get lengths of the texts\n",
      "    for worker in result:\n",
      "        texts_size += len(worker[0])\n",
      "        texts_uncorrected_size += len(worker[1])\n",
      "        texts_unlemmatized_size += len(worker[2])\n",
      "        texts_pos_size += len(worker[3])\n",
      "        lemma_dict_size += len(worker[4])\n",
      "        \n",
      "if(not feature['parallel'] or feature['preprocess_load']):\n",
      "    texts_size = len(texts)\n",
      "    texts_uncorrected_size = len(texts_uncorrected)\n",
      "    texts_unlemmatized_size = len(texts_unlemmatized)\n",
      "    texts_pos_size = len(texts_pos)\n",
      "    lemma_dict_size = len(lemma_dict)\n",
      "\n",
      "if(not feature['texts_final_load']):\n",
      "    print \"original reviews size %d\" % len(reviews)\n",
      "    print \"combined size of remote workers texts_pos %d\" % texts_pos_size\n",
      "    print \"combined size of remote workers texts_uncorrected %d\" % texts_uncorrected_size\n",
      "    print \"combined size of remote workers texts_unlemmatized %d\" % texts_unlemmatized_size\n",
      "    print \"combined size of remote workers texts %d\" % texts_size\n",
      "    # sum of len of seperate dictionaries is not necessarily equal to their original/merged size\n",
      "    print \"combined size of remote workers lemma_dict %d\" % lemma_dict_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# CELL 96\n",
      "# Merge Parallel Data back from remote workers back into one set\n",
      "if(feature['parallel'] and not (feature['preprocess_load'] or feature['texts_final_load'])):\n",
      "    texts = []\n",
      "    texts_uncorrected = []\n",
      "    texts_unlemmatized = []\n",
      "    texts_pos = []\n",
      "    lemma_dict = defaultdict(set)\n",
      "\n",
      "    # iterate through each worker and merge the texts back together\n",
      "    for worker in result:\n",
      "        texts += worker[0]\n",
      "        texts_uncorrected += worker[1]\n",
      "        texts_unlemmatized += worker[2]\n",
      "        texts_pos += worker[3]\n",
      "        for k in worker[4]:\n",
      "            lemma_dict[k] = lemma_dict[k].union(worker[4][k])\n",
      "    print \"Total data size after merge: %d\" % len(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 97\n",
      "# save data \n",
      "# write reviews, school_names, texts, texts_uncorrected, texts_unlemmatized, texts_pos, lemma_dict\n",
      "# out to a file using pickle.  This is so we don't have to re-run pre-processing later if we want to work on the same data\n",
      "if(feature['preprocess_save']):\n",
      "    filename = data_dir + '/' + file_root + '-reviews_indexes.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(reviews_indexes))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(reviews_indexes, f)\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-reviews.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(reviews))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(reviews, f)\n",
      "\n",
      "    filename = data_dir + '/' + file_root + '-school_names.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(school_names))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(school_names, f)\n",
      "        \n",
      "    filename = data_dir + '/' + file_root + '-texts.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(texts))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(texts, f)\n",
      "        \n",
      "    filename = data_dir + '/' + file_root + '-texts_uncorrected.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(texts_uncorrected))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(texts_uncorrected, f)\n",
      "        \n",
      "    filename = data_dir + '/' + file_root + '-texts_unlemmatized.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(texts_unlemmatized))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(texts_unlemmatized, f)\n",
      "        \n",
      "    filename = data_dir + '/' + file_root + '-texts_pos.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(texts_pos))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(texts_pos, f)\n",
      "        \n",
      "    filename = data_dir + '/' + file_root + '-lemma_dict.pickle'\n",
      "    print \"Writing file %s of size %d\" % (filename,len(lemma_dict))\n",
      "    with open(filename, 'w') as f:\n",
      "        pickle.dump(lemma_dict, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 98\n",
      "# Look at the text or analyze the text processing pipeline for a single review\n",
      "\n",
      "def get_review_data(review_num):\n",
      "    print \"\\nData from reviews for review number %d\" % review_num\n",
      "    print \"NCES Code: %s    Universal ID: %s\" % (reviews_indexes['nces_code'][review_num],reviews_indexes['universal_id'][review_num])\n",
      "    print \"\\n\"\n",
      "    print reviews[review_num]\n",
      "    \n",
      "    if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
      "        print \"\\nData from texts_uncorrected for review number %d\" % review_num\n",
      "        print texts_uncorrected[review_num]\n",
      "\n",
      "    if(feature['pos_tag']):\n",
      "        print \"\\nData from texts_pos for review number %d\" % review_num\n",
      "        print texts_pos[review_num]\n",
      "\n",
      "    if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
      "        print \"\\nData from texts_unlemmatized for review number %d\" % review_num\n",
      "        print texts_unlemmatized[review_num]\n",
      "\n",
      "    print \"\\nData from texts for review number %d\" % review_num\n",
      "    print texts[review_num]\n",
      "  \n",
      "if(not feature['texts_final_load']):\n",
      "    print \"Length of ingested reviews: %d\" % len(reviews)\n",
      "    print \"Length of texts_pos: %d\" % len(texts_pos)\n",
      "    print \"Length of texts_uncorrected: %d\" % len(texts_uncorrected)\n",
      "    print \"Length of texts_lemmatized: %d\" % len(texts_unlemmatized)\n",
      "    print \"Length of texts: %d\" % len(texts)\n",
      "    get_review_data(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 99\n",
      "# view descriptive statistics on our texts\n",
      "# Very interesting information here, over half the text is words that appear just 1 or 2 times!\n",
      "if(not feature['texts_final_load']):\n",
      "    if((feature['analyze_lemmatize'] and feature['lemmatize']) or (feature['analyze_spell_correct'] and feature['spell_correct'])):\n",
      "        print \"Descriptive statistics for texts_uncorrected\"\n",
      "        summary_stats(texts_uncorrected)\n",
      "    if(feature['analyze_spell_correct'] and feature['spell_correct']):\n",
      "        print \"\\nDescriptive statistics for texts_unlemmatized (after spell correction, removal of POS not in our tag_set, removal of stopwords, etc. but before lemmatization)\"\n",
      "        summary_stats(texts_unlemmatized)\n",
      "    print \"\\nDescriptive statistics for texts (lemmatized and done with pre-processing)\"\n",
      "    summary_stats(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 100\n",
      "# This cell just prints some intresting information about the lemmatization\n",
      "\n",
      "if(not feature['texts_final_load']):\n",
      "    if(feature['lemmatize'] and feature['lemma_dict']):\n",
      "        lemma_list = []\n",
      "        lemma_total = 0\n",
      "        for k,v in lemma_dict.iteritems():\n",
      "            lemma_list.append((k,list(v)))\n",
      "            lemma_total += len(v)+1\n",
      "        print \"there were %d words that were lemmatized down to a total of %d lemmas\" % (lemma_total,len(lemma_dict))\n",
      "        lemma_list = sorted(lemma_list, key = lambda x: len(x[1]), reverse=True)\n",
      "        for lemma in lemma_list:\n",
      "            print lemma"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 110\n",
      "# Create texts_bigrams\n",
      "\n",
      "# Our final text is actually a creation of the orignal unigrams we decided to keep and their bigrams.\n",
      "if(not feature['texts_final_load']):\n",
      "    texts_bigrams = [[\" \".join(bigram) for bigram in bigrams(text)] for text in texts]\n",
      "    texts_bigrams = [[bigram for bigram in text if bigram not in stopset_bigrams] for text in texts_bigrams]\n",
      "    # texts_bigrams\n",
      "    \n",
      "# Look at top 10 Bigrams (from our unigram text)\n",
      "# bcf = BigramCollocationFinder.from_words(itertools.chain.from_iterable(texts))\n",
      "# bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 150\n",
      "# examining the most frequent words, to see at what point trash is introduced\n",
      "\n",
      "def frequent_tokens(texts,topn):\n",
      "    \"\"\"\n",
      "    Prints frequency counts of tokens\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    texts : tokenized, list of lists\n",
      "        the text to analyze\n",
      "    topn : int\n",
      "        print top n frequency counts\n",
      "    \"\"\"\n",
      "    # count unigrams\n",
      "    all_tokens = list(itertools.chain.from_iterable(texts))\n",
      "    token_count = defaultdict(int)\n",
      "    for word in itertools.chain.from_iterable(texts):\n",
      "        token_count[word] += 1\n",
      "        \n",
      "    # our token_count dictionary sorted\n",
      "    sorted_freq_counts = sorted(token_count.iteritems(), key=operator.itemgetter(1))\n",
      "    \n",
      "    # just the values of our token_count dictionary so we can build a histogram\n",
      "    sorted_freq_counts_values = [item[1] for item in sorted_freq_counts]\n",
      "    \n",
      "    # print top 50 most frequent unigrams\n",
      "    for token in sorted_freq_counts[-topn:]:\n",
      "        print token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 150\n",
      "# topn most frequent unigrams\n",
      "if(not feature['texts_final_load']):\n",
      "    frequent_tokens(texts,50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 151\n",
      "# topn most frequent bigrams\n",
      "if(not feature['texts_final_load']):\n",
      "    frequent_tokens(texts_bigrams, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 160\n",
      "# Load texts_final\n",
      "if(feature['texts_final_load']):\n",
      "    with open(data_dir + '/' + file_root + '-texts_final' + '.pickle') as f:\n",
      "        texts_final = pickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 170\n",
      "# Stitch unigrams and bigrams together in a new text texts_combined\n",
      "\n",
      "if(not feature['texts_final_load']):\n",
      "    texts_combined = []\n",
      "    for index,text in enumerate(texts):\n",
      "        texts_combined.append(text+texts_bigrams[index])\n",
      "    \n",
      "    print len(texts_combined)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 175\n",
      "# Choose which text we will be processing with our model:\n",
      "#    unigrams = texts\n",
      "#    bigrams  = texts_bigrams\n",
      "#    unigrams + bigrams = texts_combined\n",
      "\n",
      "if(not feature['texts_final_load']):\n",
      "    # texts_final = texts\n",
      "    # texts_final = texts_bigrams\n",
      "    texts_final = texts_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 178\n",
      "# Save texts_final\n",
      "\n",
      "if(feature['texts_final_save']):\n",
      "    with open(data_dir + '/' + file_root + '-texts_final' + '.pickle', 'w') as f:\n",
      "        pickle.dump(texts_final, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 180\n",
      "# create a dictionary of words and filter it\n",
      "# essentially makeing a N-D vector representation\n",
      "\n",
      "if(not feature['dict_corpus_load']):\n",
      "\n",
      "    # assigns integer id's to each word along with word counts and other statistics\n",
      "    dictionary = corpora.Dictionary(texts_final)\n",
      "\n",
      "    # filter dictionary, still need to find optimal settings default is no filtering\n",
      "    # Currently trying this at no_below=5, no_above=0.6, keep_n=None\n",
      "    # filters\n",
      "    # 1. less than no_below documents (absolute number) or\n",
      "    # 2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
      "    # after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
      "    # After the pruning, shrink resulting gaps in word ids. (this means word ids may change after gap shrinking)\n",
      "    dictionary.filter_extremes(no_below=5, no_above=0.6, keep_n=None)\n",
      "    # print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 182\n",
      "# save dictionary to disk for later use\n",
      "\n",
      "if(feature['dict_corpus_save']):\n",
      "    dictionary.save(data_dir + '/' + file_root + '.dict') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 190\n",
      "# look at mappings between id's and words\n",
      "\n",
      "# print dictionary.token2id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 200\n",
      "# test our dictionary\n",
      "\n",
      "# we take the string to lower() and split on whitespace.  If this example had more complex things like punctuation, we would\n",
      "# need to handle it like I did earlier with the full text reviews......this is just a simple example.\n",
      "# the words \"has\" and \"and\" are ignored because these are stopwords, and were filtered and never added to the dictionary \n",
      "# in the first place.  The word \"sauropod\" does not appear in the dictionary because it was not in any review we ingested.\n",
      "# id mappings below may be different from actual at this time\n",
      "# 51 is \"school\", 57 is \"students\", 61 is \"teachers\"\n",
      "# The ,1 after each id is the word count, which in this case is 2 for students and 1 for others \n",
      "# Note: now that I do dictionary filtering, the above words are filtered out, so the example is not the same as\n",
      "# what you will see below\n",
      "# new_doc = \"military organizes disgrace and sauropods\"\n",
      "# new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
      "# print new_vec "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 205\n",
      "# load a previous saved dictionary and corpus\n",
      "\n",
      "if(feature['dict_corpus_load']):\n",
      "\n",
      "    dictionary = corpora.Dictionary.load(data_dir + '/' + file_root + '.dict')\n",
      "\n",
      "    # do not re-filter if the dictionary you are loading was already previously filtered before it was saved \n",
      "    # dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
      "    # print dictionary\n",
      "\n",
      "    corpus_bow = corpora.MmCorpus(data_dir + '/' + file_root + '.mm')\n",
      "    # print corpus_bow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 210\n",
      "# our text converted to a bag of words based on our dictionary\n",
      "if(not feature['dict_corpus_load']):\n",
      "    corpus_bow = [dictionary.doc2bow(text) for text in texts_final]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 212\n",
      "# save corpus to disk for later use\n",
      "if(feature['dict_corpus_save']):\n",
      "    corpora.MmCorpus.serialize(data_dir + '/' + file_root + '.mm', corpus_bow) \n",
      "    # print corpus_bow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 230\n",
      "# How many times does a particular word appear in the corpus?\n",
      "\n",
      "# Look up the id of the token \n",
      "# id = dictionary.token2id['military']\n",
      "\n",
      "# Now get the count for that id\n",
      "# total_sum = sum(dict(doc).get(123, 0) for doc in corpus_bow)\n",
      "# print total_sum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 240\n",
      "\n",
      "# create a TF-IDF model of our corpus\n",
      "# this model can now convert data that was represented as \"bag of words\" to the new TF-IDF representation\n",
      "# From the documentation \"Expects a bag-of-words (integer values) training corpus during initialization. During transformation, \n",
      "# it will take a vector and return another vector of the same dimensionality, except that features which were rare in the training \n",
      "# corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving the \n",
      "# number of dimensions intact. It can also optionally normalize the resulting vectors to (Euclidean) unit length.\"\n",
      "# Note, I did not normalize here but I could.\n",
      "if(feature['tfidf']):\n",
      "    model_tfidf = models.TfidfModel(corpus_bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 250\n",
      "# view a bow in our tfidf corpus\n",
      "# for example I take review 1\n",
      "# doc_bow = [(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 4), \n",
      "#           (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 2), \n",
      "#           (25, 1), (26, 1), (27, 2), (28, 2), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), \n",
      "#           (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), \n",
      "#           (49, 1), (50, 1), (51, 3), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 3), (58, 1), (59, 2), (60, 1), \n",
      "#           (61, 3), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 2)]\n",
      "# print model_tfidf[doc_bow]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 260\n",
      "# transform the entire corpus to TF-IDF\n",
      "if(feature['tfidf']):\n",
      "    corpus_tfidf = model_tfidf[corpus_bow]\n",
      "\n",
      "# This can take a while to print (15m or so)\n",
      "# for doc in corpus_tfidf:\n",
      "#    print doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 265\n",
      "# choose a corpus\n",
      "if(feature['tfidf']):\n",
      "    corpus = corpus_tfidf\n",
      "else:\n",
      "    corpus = corpus_bow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 268\n",
      "# perplexity grid search\n",
      "# we will take our documents, divide them into 80% train and 20% test.  We will measure the test data against the model built with \n",
      "# train  and calculate a perplexity measurement.  The goal will be to find the parameters which minimize perplexity.\n",
      "# Intution gathered from posts on the gensim mailing list including https://groups.google.com/forum/#!topic/gensim/tsGNoDkMY7U\n",
      "\n",
      "if(feature['perplexity_search']):\n",
      "    \n",
      "    grid = defaultdict(list)\n",
      "\n",
      "    # Choose a parameter you are wanting to search, for example num_topics or alpha / eta, make sure you substitute \"parameter_value\"\n",
      "    # into the model below instead of a static value.\n",
      "    #\n",
      "    # num topics\n",
      "    # parameter_list=[10, 25, 50, 75, 100]\n",
      "    \n",
      "    # alpha / eta\n",
      "    parameter_list=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5]\n",
      "    \n",
      "    # shuffle corpus\n",
      "    cp = list(corpus)\n",
      "    random.shuffle(cp)\n",
      "\n",
      "    # split into 80% training and 20% test sets\n",
      "    p = int(len(cp) * .8)\n",
      "    cp_train = cp[0:p]\n",
      "    cp_test = cp[p:]\n",
      "\n",
      "    # for num_topics_value in num_topics_list:\n",
      "    for parameter_value in parameter_list:\n",
      "\n",
      "        # print \"starting pass for num_topic = %d\" % num_topics_value\n",
      "        print \"starting pass for parameter_value = %.3f\" % parameter_value\n",
      "        start_time = time.time()\n",
      "\n",
      "        # run model\n",
      "        model = models.ldamodel.LdaModel(corpus=cp_train, id2word=dictionary, num_topics=40, chunksize=2000, \n",
      "                                        passes=50, update_every=0, alpha=parameter_value, eta=parameter_value, decay=0.5,\n",
      "                                        distributed=True)\n",
      "    \n",
      "        # show elapsed time for model\n",
      "        elapsed = time.time() - start_time\n",
      "        print \"Elapsed time: %s\" % elapsed\n",
      "    \n",
      "        perplex = model.bound(cp_test)\n",
      "        print \"Perplexity: %s\" % perplex\n",
      "        grid[parameter_value].append(perplex)\n",
      "    \n",
      "        per_word_perplex = np.exp2(-perplex / sum(cnt for document in cp_test for _, cnt in document))\n",
      "        print \"Per-word Perplexity: %s\" % per_word_perplex\n",
      "        grid[parameter_value].append(per_word_perplex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 269\n",
      "# View the results of our grid search\n",
      "if(feature['perplexity_search']):\n",
      "    for parameters in grid:\n",
      "        print parameters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# CELL 270\n",
      "# Choose a model\n",
      "\n",
      "# took 24m 57s to do LDA num_topics=40, chunksize=500, passes=100, update_every=0, distributed=True \n",
      "# on my 4 core 2.66Ghz mac pro, on 37349 reviews using 8 virtual cores (8 workers)\n",
      "\n",
      "# If you don't have gensim setup for distributed set distributed=False\n",
      "# update_every is set to 0 as we are running in \"batch\" mode vs. online\n",
      "# chunksize is ideally = num documents / num workers     (when running in batch mode)\n",
      "# alpha and eta defaul to 1/num_topics if not set (None)\n",
      "if(not feature['model_load']):\n",
      "    model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, chunksize=100, \n",
      "                                  passes=1, update_every=0, alpha=None, eta=None, decay=0.5,\n",
      "                                  distributed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 289\n",
      "# Load model\n",
      "\n",
      "if(feature['model_load']):\n",
      "    model = models.ldamodel.LdaModel.load(data_dir + '/' + file_root + '-model-t' + str(num_topics) + '.lda')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 290\n",
      "# Save model\n",
      "\n",
      "if(feature['model_save']):\n",
      "    model.save(data_dir + '/' + file_root + '-model-t' + str(num_topics) + '.lda') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 292\n",
      "# Build corpus model\n",
      "\n",
      "# create a double wrapper over the original corpus: bow->fold-in-lda, bow->tfidf->fold-in-lda, etc.\n",
      "# this is so you can view how documents are classified by the model\n",
      "if(not feature['corpus_model_load']):\n",
      "    corpus_model = model[corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 293\n",
      "# Load corpus model\n",
      "\n",
      "if(feature['corpus_model_load']):\n",
      "    with open(data_dir + '/' + file_root + '-corpus-model-t' + str(num_topics) + '.pickle') as f:\n",
      "        corpus_model = pickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 295\n",
      "# View corpus model\n",
      "\n",
      "# you can leave this commented unless you wish to view the contents of corpus_model in its entirity\n",
      "# I don't recommend it, because the transforms are done on the fly and it takes about 15min\n",
      "# Here you can see each review, and how it relates to each of the n topics.  Each list is a review (document) and each item \n",
      "# in the list is a tuple of (topic number,score)\n",
      "# the actual transformations, for example bow->lda are actually executed here, on the fly\n",
      "# for doc in corpus_model: \n",
      "#    print doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 296\n",
      "# Save corpus model\n",
      "\n",
      "if(feature['corpus_model_save']):\n",
      "    with open(data_dir + '/' + file_root + '-corpus-model-t' + str(num_topics) + '.pickle', 'w') as f:\n",
      "        pickle.dump(corpus_model, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 300\n",
      "# load topics\n",
      "if(feature['model_topics_load']):\n",
      "    with open(data_dir + '/' + file_root + '-topics-t' + str(num_topics) + '.pickle') as f:\n",
      "        model_topics = pickle.load(f)\n",
      "    for num,topic in enumerate(model_topics[0]):\n",
      "        print num,topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 301\n",
      "# print topics\n",
      "model_topics = model.print_topics(num_topics)\n",
      "model_topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 302\n",
      "# save topics\n",
      "if(feature['model_topics_save']):\n",
      "    with open(data_dir + '/' + file_root + '-topics-t' + str(num_topics) + '.pickle', 'w') as f:\n",
      "        pickle.dump(model_topics, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 310\n",
      "# These functions copied from https://groups.google.com/forum/#!searchin/gensim/visualize/gensim/SxFKsSsBTRs/cN6p3XaH4rUJ\n",
      "# They extract the gamma, beta and log probabilities \n",
      "\n",
      "def get_gamma(lda, corpus):\n",
      "    \"\"\"\n",
      "    Return gamma from a gensim LdaModel instance.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    lda : LdaModel\n",
      "        A fitted model.\n",
      "    corpus : gensim Corpus\n",
      "        An iterable Bag-of-Words Corpus used to fit the LDA model\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    gamma : ndarray\n",
      "        An ndarray that contains gamma.\n",
      "    \"\"\"\n",
      "    # lda.VAR_MAXITER = 'est' \n",
      "    chunksize = lda.chunksize\n",
      "    chunker = itertools.groupby(enumerate(corpus),\n",
      "               key=lambda (docno, doc): docno/chunksize)\n",
      "    all_gamma = []\n",
      "    for chunk_no, (key, group) in enumerate(chunker):\n",
      "        chunk = np.asarray([np.asarray(doc) for _, doc in group])\n",
      "        (gamma, sstats) = lda.inference(chunk)\n",
      "        all_gamma.append(gamma)\n",
      "    return np.vstack(all_gamma)\n",
      "\n",
      "def blei_gamma(fname, gamma):\n",
      "    \"\"\"\n",
      "    Writes the gamma file in Blei's format.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    fname : str\n",
      "        Path to file to save to\n",
      "    gamma : ndarray\n",
      "        Numpy array returned from get_gamma\n",
      "    \"\"\"\n",
      "    np.savetxt(fname, gamma, fmt='%5.10f')\n",
      "\n",
      "def blei_beta(fname, lda):\n",
      "    \"\"\"\n",
      "    Write log probabilities to a space-delimited file as lda-c final.beta\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    fname : str\n",
      "        Filename\n",
      "    lda : LdaModel\n",
      "        LdaModel instance\n",
      "    \"\"\"\n",
      "    expElogbeta = np.log(lda.expElogbeta)\n",
      "    np.savetxt(fname, expElogbeta, fmt='%5.10f')\n",
      "    \n",
      "if(feature['beta_gamma_save']):\n",
      "    blei_beta(data_dir + '/' + file_root + '-t' + str(num_topics) + '.beta', model)\n",
      "    gamma = get_gamma(model, corpus)\n",
      "    blei_gamma(data_dir + '/' + file_root + '-t' + str(num_topics) + '.gamma', gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 1000\n",
      "# Exploratory things you can try\n",
      "\n",
      "# set a review number to examine\n",
      "review_num=0\n",
      "\n",
      "def get_topic_data(review_num):\n",
      "    \"\"\"\n",
      "    Produces information about topics associated to a particular review\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    review_num : int\n",
      "        a valid review number \n",
      "    \"\"\"\n",
      "    \n",
      "    # Look at the topics associated to this review in the corpus_model\n",
      "    print \"\\nTopics for review number %d\" % review_num\n",
      "    topics = model[corpus[review_num]]\n",
      "    print topics\n",
      "    print \"\\nTopic details for topics in review number %d\" % review_num\n",
      "    topic_info = [model.print_topic(topic[0],topn=10) for topic in topics]\n",
      "    for topic in topic_info:\n",
      "        print topic\n",
      "\n",
      "# Look at this review in the corpus (bag of words, tf-idf, etc)\n",
      "# print corpus[review_num]\n",
      "\n",
      "# get data about the document as it moved through the pipeline\n",
      "if(not feature['texts_final_load']):\n",
      "    get_review_data(review_num)\n",
      "get_topic_data(review_num)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# CELL 1100 \n",
      "# Create CSV file to place topic numbers into the database for each review\n",
      "\n",
      "import csv\n",
      "with open(data_dir + '/' + file_root + '-review_topics-t' + str(num_topics) + '.csv', 'wb') as csvfile:\n",
      "    topicwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
      "    \n",
      "    for row in xrange(len(reviews)):\n",
      "        # we store gsid, nces_code and universal_id in topic_array_1 which is str (some nces_codes are alphanum)\n",
      "        topic_array_1 = np.empty(3, dtype=\"S10\")\n",
      "        # we store each topic percentage in topic_array_2 which is floats\n",
      "        topic_array_2 = np.zeros(num_topics, float)\n",
      "        \n",
      "        if(row % 1000 == 0):\n",
      "            print \"Processing row %d\" % row\n",
      "        gsid, nces_code, universal_id, postdate = reviews_indexes.irow(row)\n",
      "        # some gsid's are None because the school had no reviews\n",
      "        if(gsid is None):\n",
      "            gsid = 999999999\n",
      "        topic_array_1[0], topic_array_1[1], topic_array_1[2] = gsid, nces_code, universal_id\n",
      "\n",
      "        # we get the topics from the model\n",
      "        topics = model[corpus[row]]\n",
      "    \n",
      "        # we update the topic_array_2 with the percentage values for each topic\n",
      "        for topic in topics:\n",
      "            topic_num,topic_score = topic\n",
      "            topic_array_2[topic_num] = topic_score\n",
      "         \n",
      "        # we combine both arrays into a single writerow\n",
      "        topicwriter.writerow(list(topic_array_1) + list(topic_array_2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filename = data_dir + '/' + 'all_schools-texts_final.pickle'\n",
      "# with open(filename) as f:\n",
      "#    texts_final = pickle.load(f)\n",
      "# print \"Loaded file %s of size %d\" % (filename,len(texts_final))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# f = open(data_dir + '/' + 'all_schools-texts_final.txt','w')\n",
      "# for review in texts_final:\n",
      "#  f.write(\"%s\\n\" % review)\n",
      "# f.close() # you can omit in most cases as the destructor will call if"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = [1,2,3,4,5]\n",
      "good_indexes = [0,2,4]\n",
      "\n",
      "foo = [test[i] for i in good_indexes]\n",
      "print foo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 1500 \n",
      "# Word Frequency Counts\n",
      "# remove encodings, tokenize, remove punctuation, remove stopwords, remove words < 3, remove non alpha words\n",
      "\n",
      "@dv.remote(block=True)\n",
      "def freq_clean():\n",
      "\n",
      "    texts = []\n",
      "\n",
      "    counter = 0\n",
      "\n",
      "    # we need to strip escape characters so we compile a pattern\n",
      "    hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
      "        \n",
      "    print \"Starting to process %d documents\" % len(data_to_clean)\n",
      "\n",
      "    for review_text in data_to_clean:\n",
      "        \n",
      "        # print result to stdout, note that when using parallel stdout is not sent to the Client\n",
      "        # counter += 1\n",
      "        # if ((counter % 1000)==0):\n",
      "        #    print \"%d documents processed\" % counter\n",
      "        # sys.stdout.flush()\n",
      "                          \n",
      "        # remove html encodings\n",
      "        review_text = review_text.replace('&amp;','').replace('&lt;','').replace('&gt;','').replace('&quot;','').replace('&#039;','').replace('&#034;','')\n",
      "        review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
      "      \n",
      "        # tokenize, you don't want to turn this off\n",
      "        review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
      "\n",
      "        # remove punctuation\n",
      "        review_text = [word.translate(string.maketrans(\"\",\"\"), string.punctuation) for word in review_text]\n",
      "                    \n",
      "        # remove smallwords\n",
      "        review_text = [word for word in review_text if not len(word) < 3]\n",
      "        \n",
      "        # lowercase all text\n",
      "        review_text = [word.lower() for word in review_text]\n",
      "        \n",
      "        # remove stopwords\n",
      "        review_text = [word for word in review_text if not word in stopset_to_use]\n",
      "        \n",
      "        texts.append(review_text)\n",
      "    return texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 1510\n",
      "# Merge Freq Count data back to one variable\n",
      "\n",
      "def freq_merge(result):\n",
      "    # Merge Parallel Data back from remote workers back into one set\n",
      "    texts = []\n",
      "    \n",
      "    # iterate through each worker and merge the texts back together\n",
      "    for worker in result:\n",
      "        texts += worker\n",
      "    print \"Total after merge: %d\" % len(texts)\n",
      "    return texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 1520\n",
      "# get reviews for a single state\n",
      "\n",
      "def get_state(state):\n",
      "    cnx = mysql_ops()\n",
      "    #if(MAX_NUM_REVIEWS == 0):\n",
      "    reviews = cnx.get_reviews_state(state)[['reviews']]\n",
      "    # else:\n",
      "    #    reviews = cnx.get_reviews_state(state)[['reviews']][:MAX_NUM_REVIEWS]\n",
      "\n",
      "    print \"Ingested %d documents from database for state %s\" % (len(reviews),state)\n",
      "    return reviews\n",
      "\n",
      "def get_all():\n",
      "    cnx = mysql_ops()\n",
      "    #if(MAX_NUM_REVIEWS == 0):\n",
      "    reviews = cnx.get_all_reviews()\n",
      "    # else:\n",
      "    #    reviews = cnx.get_all_reviews()[['reviews']][:MAX_NUM_REVIEWS]\n",
      "\n",
      "    # reviews = reviews['reviews']\n",
      "    print \"Ingested %d documents from database for all states\" % len(reviews)\n",
      "    return reviews"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CELL 1530\n",
      "# write freq counts to file\n",
      "\n",
      "def freq_write(texts,state):\n",
      "    f = open(data_dir + '/' + state + '-freq.txt','w')\n",
      "    print \"Writing file for state %s\" % state\n",
      "    fdist = nltk.FreqDist(list(itertools.chain.from_iterable(texts)))\n",
      "    for word,count in fdist.items()[:200]:\n",
      "        f.write(\"%s,%s\\n\" % (word,count))\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews_all = get_all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ingested 863894 documents from database for all states\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stars_0 = reviews_all[reviews_all['stars'] == 99]\n",
      "# stars_1 = reviews_all[reviews_all['stars'] == 1]\n",
      "# stars_2 = reviews_all[reviews_all['stars'] == 2]\n",
      "# stars_3 = reviews_all[reviews_all['stars'] == 3]\n",
      "# stars_4 = reviews_all[reviews_all['stars'] == 4]\n",
      "# stars_5 = reviews_all[reviews_all['stars'] == 5]\n",
      "# reviewer_parent = reviews_all[reviews_all['reviewer'] == 'parent']\n",
      "# reviewer_student = reviews_all[reviews_all['reviewer'] == 'student']\n",
      "# reviewer_other = reviews_all[reviews_all['reviewer'] == 'other']\n",
      "# reviewer_teacher = reviews_all[reviews_all['reviewer'] == 'teacher']\n",
      "# reviewer_former_student = reviews_all[reviews_all['reviewer'] == 'former student']\n",
      "# reviewer_empty = reviews_all[reviews_all['reviewer'] == '']\n",
      "# reviewer_staff = reviews_all[reviews_all['reviewer'] == 'staff']\n",
      "# reviewer_administrator = reviews_all[reviews_all['reviewer'] == 'administrator']\n",
      "# reviewer_principal = reviews_all[reviews_all['reviewer'] == 'principal']\n",
      "# type_public = reviews_all[reviews_all['type'] == 'public']\n",
      "# type_private = reviews_all[reviews_all['type'] == 'private']\n",
      "# type_charter = reviews_all[reviews_all['type'] == 'charter']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print len(stars_0)\n",
      "# print len(stars_1)\n",
      "# print len(stars_2)\n",
      "# print len(stars_3)\n",
      "# print len(stars_4)\n",
      "# print len(stars_5)\n",
      "# print len(reviewer_parent)\n",
      "# print len(reviewer_student)\n",
      "# print len(reviewer_other)\n",
      "# print len(reviewer_teacher)\n",
      "# print len(reviewer_former_student)\n",
      "# print len(reviewer_empty)\n",
      "# print len(reviewer_staff)\n",
      "# print len(reviewer_administrator)\n",
      "# print len(reviewer_principal)\n",
      "# print len(type_public)\n",
      "# print len(type_private)\n",
      "# print len(type_charter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "601075\n",
        "194763\n",
        "68056\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print reviews_all['type'].value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "public     601075\n",
        "private    194763\n",
        "charter     68056\n",
        "dtype: int64\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "# CELL 1540\n",
      "# main pipeline to process freq counts\n",
      "\n",
      "states = ['AK','AL','AR','AZ','CA','CO','CT','DC','DE','FL',\n",
      "          'GA','HI','IA','ID','IL','IN','KS','KY','LA','MA',\n",
      "          'MD','ME','MI','MN','MO','MS','MT','NC','ND','NE',\n",
      "          'NH','NJ','NM','NV','NY','OH','OK','OR','PA','PR',\n",
      "          'RI','SC','SD','TN','TX','UT','VA','VT','WA','WI',\n",
      "          'WV','WY']\n",
      "\n",
      "# for state in states:\n",
      "state = 'type_charter'\n",
      "# print \"Processing state %s\" % state\n",
      "# reviews = get_state(state)\n",
      "# reviews = get_all()\n",
      "\n",
      "reviews = type_charter['reviews']\n",
      "\n",
      "data_to_clean = reviews\n",
      "stopset_to_use = set(nltk.corpus.stopwords.words('english'))\n",
      "\n",
      "# copy the data_to_clean and stopset_to_use into name space of remote workers\n",
      "    \n",
      "# We use ipythons parallel processing \"scatter\" to shard \"reviews\" accross our cores\n",
      "dv.scatter('data_to_clean',data_to_clean)\n",
      "\n",
      "# We copy the objects feature and stopset, as well as the Class SpellingReplacer to our remote workers\n",
      "dv['stopset_to_use'] = stopset_to_use\n",
      "\n",
      "# Show size of entire data being sent for processing\n",
      "print \"Total data size: %d\" % len(data_to_clean)\n",
      "\n",
      "# Show the shard size of each remote worker\n",
      "print \"Sharded data size for each worker\"\n",
      "%px print len(data_to_clean)\n",
      "\n",
      "# clean the reviews\n",
      "result = freq_clean()\n",
      "\n",
      "# merge results back\n",
      "texts = freq_merge(result)\n",
      "\n",
      "# write file \n",
      "freq_write(texts,state)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total data size: 68056\n",
        "Sharded data size for each worker\n",
        "[stdout:0] 2127\n",
        "[stdout:1] 2127\n",
        "[stdout:2] 2127\n",
        "[stdout:3] 2127\n",
        "[stdout:4] 2127\n",
        "[stdout:5] 2127\n",
        "[stdout:6] 2127\n",
        "[stdout:7] 2127\n",
        "[stdout:8] 2127\n",
        "[stdout:9] 2127\n",
        "[stdout:10] 2127\n",
        "[stdout:11] 2127\n",
        "[stdout:12] 2127\n",
        "[stdout:13] 2127\n",
        "[stdout:14] 2127\n",
        "[stdout:15] 2127\n",
        "[stdout:16] 2127\n",
        "[stdout:17] 2127\n",
        "[stdout:18] 2127\n",
        "[stdout:19] 2127\n",
        "[stdout:20] 2127\n",
        "[stdout:21] 2127\n",
        "[stdout:22] 2127\n",
        "[stdout:23] 2127\n",
        "[stdout:24] 2126\n",
        "[stdout:25] 2126\n",
        "[stdout:26] 2126\n",
        "[stdout:27] 2126\n",
        "[stdout:28] 2126\n",
        "[stdout:29] 2126\n",
        "[stdout:30] 2126\n",
        "[stdout:31] 2126\n",
        "Total after merge: 68056"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Writing file for state type_charter\n",
        "CPU times: user 12.2 s, sys: 104 ms, total: 12.3 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 17.9 s\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}